{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Project - combining geometric and semantic distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "\n",
    "Ivan da Silva Bras√≠lico\n",
    "\n",
    "April 23, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Background\n",
    "\n",
    "There are many aproachs for adressing image similarity search. The old ones use hand crafted features, like HOG, SIFT or Histograms of images, and some distance, like Euclidean Distances, to compare these features. More modern aproaches uses Neural Networks.\n",
    "\n",
    "Two common image similarity search methods are using autoencoders to generate a compact representation of the image, or accessing this compact representation on features learned on Vanilla Networks pre-trained on ImageNet (i.e., using values from the latter convolutional filters on pre-trained Nets like VGG, DenseNet, Inception, etc). The autoencoder method tends to learn the geometric/exact match. The convolutional filter can, depending on the layer used, be more geometric or more semantic. These methods commonly use euclidean distances to compare the generated features between target images and the image database.\n",
    "\n",
    "There are alternative aproachs, like, for fast search, as proposed by Hinton, using Markov Models to learn binary patches and use hamming distance, or even memory addresses, for very fast searching (https://pdfs.semanticscholar.org/bba0/6d9f6632391313f62143596342945cbf7a0e.pdf). Also, one can generate text labels for images (image captioning https://arxiv.org/pdf/1810.04020.pdf) and use text retrieval methods (bag of words, tf/idf) to do semantic similarity search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "All the cited methods have their strenghts, but for the problem that I have in mind, I feel like that I need a different method. I have greyscale Container Cargo X-Ray Images, and these pre-trained models are trained with color photographs, very different images from what we see in papers and contests on the Internet and open courses. Also, **I need a search that can capture a set of characteristics: semantic, geometry, packing, density zones, patterns, and is position invariant and translation invariant**. Also, I have some images that are very similar, but not equal, like the same cargo (HS Code) but with different quantity, size or qualities.\n",
    "\n",
    "Then, I need some method that trains comparing an image with itself (like in autoencoders and hinton deep belief markov models), with its semantic (like in convolutional networks used to object detection and classification), and with its \"cousins\", images that are not equal but have the same cargo (like siamese models trained to classify mnist and fashion-mnist).\n",
    "\n",
    "Clarifying, I need a similarity search that, when there is a seizure, allows the fiscalization agents that are in cargo of X-ray container scanners to use the image of the problematic cargo to find other cargo that have the same menace (like a smugling, or even drugs/guns ocultation). Or may find anomalies in a lot, that was expected to be very similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Inputs\n",
    "\n",
    "I have about 1.5 million X-Ray images, half in VGA, half in SVGA resolution. All of these were associated, by a script, with the Bill of Lading related information, that contains some information, like description of goods contained, weight, volume, HS Code, and others. Also, quite a few (only hundreds) are manually rotulated because there was a seizure (smuggling, cocaine, others).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Statement\n",
    "\n",
    "Using Siamese convolutuional netwoks, try to create an image similarity search engine that can do well on:\n",
    "\n",
    "* Detecting images from same lot/HS Code\n",
    "* Detecting lot anomalies\n",
    "* Finding images similar to a cargo seized\n",
    "* Comparing same cargo scanning images from different location and alarming when cargo was touched/manipulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model\n",
    "\n",
    "\n",
    "I already have a very simple full conected autoencoder on production. This model will be the baseline. Also, a convolutional autoencoder can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "\n",
    "The simpler evaluation metric will be the capacity with indexing the same image, with litle alterations (contrast), an small area modified, and images of the same lot/HS Code, in close positions. The train sets will be produced like these, and a human evaluator will need to see if the imagens from a lot/HS Code are really close before the training procedure. The metric will consider if images from a lot/HS Code are classified togheter and from distant lots/HS Codes are not.\n",
    "\n",
    "Another evaluation metric will be comparing with the actual autoencoder results.\n",
    "\n",
    "One evaluation will need to use human avaliation, like on Text Rank methods. The searcher will obtain a list of ranked search results and an human evaluator will give some grade, that will be multiplied by the rank position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Design\n",
    "\n",
    "First, a script will be designed to generate a copy of the images on disk, in series of folders named on HSCode\n",
    "\n",
    "Then, these folders will be inspected (first by the autoencoder and then visually) to see if the images on folder really have similarity.\n",
    "\n",
    "After that, a very simple Convolutional Network will be designed, initialized and trained from the ground, to validate the expectations. This network will be mounted on a Siamese configuration, and trained to recognize images from the same HSCode (The prepared folders). The last layers of the convolutional network will, hopefully, generate features that can say if an image is similar to each other, constrained to the details that make images from same HSCodes be similar.\n",
    "\n",
    "These features extracted on last layers can than be reduced via PCA or T-SNE and visualized. Also, they can be used to create an Index for image search, and that index can be visually evaluated and compared with baseline model.\n",
    "\n",
    "If the results look promising, then the same mouting and training method can be repeated, this time using a more sofisticaded network, like DenseNet, to see if better results can be achieved. Then, the trained network can be used to generate and save a \"Hash\" for every image from the features extracted, so it can be used to develop a image search application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padma-venv",
   "language": "python",
   "name": "padma-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
